{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a62d887-01aa-4980-9220-3bf28c077504",
   "metadata": {},
   "source": [
    "### Домашнее задание Transformers Training (50 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d7452-febe-4d25-a9c1-005fcc26b35b",
   "metadata": {},
   "source": [
    "В этом домашнем задании требуется обучить несколько Transformer-based моделей в задаче машинного перевода. Для обучения можно воспользоваться текущим проектом, так и реализовать свой пайплайн обучения. Если будете использовать проект, теги **TODO** проекта отмечают, какие компоненты надо реализовать.\n",
    "В ноутбуке нужно только отобразить результаты обучения и выводы. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Ваш код обучения нужно выложить на ваш github, в строке ниже дать ссылку на него. В первую очередь будут оцениваться результаты в ноутбуке, код нужен для проверки адекватности результатов. \n",
    "\n",
    "Обучать модели до конца не нужно, только для демонстрации, что модель обучается и рабочая - снижение val_loss, рост bleu_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691e7b9-6538-42b1-96b8-838f2efab4af",
   "metadata": {},
   "source": [
    "#### Сcылка на ваш github с проектом(вставить свой) - https://github.com/runnerup96/pytorch-machine-translation\n",
    "\n",
    "Ноутбук с результатами выкладывать на ваш **google диск** курса. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca1e62-b210-4426-9854-55b652417a4b",
   "metadata": {},
   "source": [
    "### Данные\n",
    "\n",
    "`\n",
    "wget https://www.manythings.org/anki/rus-eng.zip && unzip rus-eng.zip\n",
    "`\n",
    "\n",
    "Модели нужно обучить на задаче перевода с английского на русский. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e84de-f7a4-46da-a611-74c90a67e24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc772a6e-7d1a-4d8d-8024-0454a948835b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Обучение Seq2seq Transformer модель(25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Transformer. В качестве блока трансформера можно использовать https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html. В качестве токенизатора воспользуйтесь HuggingFace токенизатор для source/target языков - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. \n",
    "\n",
    "Не забудьте остальные элементы модели:\n",
    "* Мы можем использовать 1 трансформер как энкодер - декодером будет выступать линейный слой. \n",
    "* Обучите свой BPE токенизатор - https://huggingface.co/docs/transformers/fast_tokenizers\n",
    "* Матрицу эмбеддингов токенов\n",
    "* Матрицу позицонных эмбеддингов\n",
    "* Линейный слой проекции в target словарь\n",
    "* Функцию маскирования будущих состояний attention, так как модель авто-регрессионна\n",
    "* Learning rate schedualer\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, размерность скрытого слоя, количетсво слоев\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5c8f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from file: 100%|██████████| 496059/496059 [00:06<00:00, 72247.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры предложений:\n",
      "go  -->  марш\n",
      "go  -->  иди\n",
      "go  -->  идите\n"
     ]
    }
   ],
   "source": [
    "from src.data.utils import TextUtils, short_text_filter_function\n",
    "\n",
    "def prepare_data(filepath, max_length=15):\n",
    "    pairs = TextUtils.read_langs_pairs_from_file(filename=filepath)\n",
    "    \n",
    "    filtered_pairs = [pair for pair in pairs if short_text_filter_function(pair, max_length=max_length)]\n",
    "    \n",
    "    english_sentences = [pair[0] for pair in filtered_pairs]\n",
    "    russian_sentences = [pair[1] for pair in filtered_pairs]\n",
    "\n",
    "    return english_sentences, russian_sentences\n",
    "\n",
    "english, russian = prepare_data('data/rus.txt')\n",
    "print(\"Примеры предложений:\")\n",
    "for i in range(3):\n",
    "    print(english[i], \" --> \", russian[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a516a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры предложений:\n",
      "there s a huge difference between making oneself understood and completely mastering english  -->  есть огромная разница между быть понятным другим и правильно говорить по английски\n",
      "every student of biology anatomy anthropology ethnology or psychology is familiar with these facts  -->  каждый студент изучающий биологию анатомию антропологию этнологию и психологию знаком с этими фактами\n"
     ]
    }
   ],
   "source": [
    "print(\"Примеры предложений:\")\n",
    "for i in range(-3, -1):\n",
    "    print(english[i], \" --> \", russian[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765ec179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Anaconda3\\envs\\ML311\\Lib\\site-packages\\torchtext\\data\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from src.data.bpe_tokenizer import BPETokenizer\n",
    "from src.data.datamodule import DataManager\n",
    "from src.models.seq2seq_transformer import Seq2SeqTransformer\n",
    "from src.models.trainer import Trainer\n",
    "from src.txt_logger import TXTLogger\n",
    "from tokenizers import Tokenizer\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c110390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_tokenizer = Tokenizer.from_file(\"source_tokenizer.json\")\n",
    "# target_tokenizer = Tokenizer.from_file(\"target_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14eee6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6506368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from file: 100%|██████████| 496059/496059 [00:06<00:00, 73767.22it/s]\n"
     ]
    }
   ],
   "source": [
    "data_manager = DataManager(\n",
    "    config={\n",
    "        \"prefix_filter\": [],\n",
    "        \"filename\": 'data/rus.txt',\n",
    "        \"max_length\": 15,\n",
    "        \"train_size\": 0.8,\n",
    "        \"batch_size\": 128\n",
    "    },\n",
    "    device=DEVICE\n",
    ")\n",
    "train_dataloader, val_dataloader = data_manager.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7ab6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqTransformer(device=DEVICE,\n",
    "                          embedding_size=256,\n",
    "                          num_encoder_layers=8,\n",
    "                          dim_feedforward=4096,\n",
    "                          src_voc_size=len(data_manager.source_tokenizer.tokenizer.get_vocab()),\n",
    "                          trg_voc_size=len(data_manager.target_tokenizer.tokenizer.get_vocab()),\n",
    "                          target_tokenizer=data_manager.target_tokenizer,\n",
    "                          source_tokenizer=data_manager.source_tokenizer,\n",
    "                          lr_decay_step=5,\n",
    "                          lr = 1e-4,\n",
    "                          lr_decay=1e-1\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d103972",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    'epoch_num': 10\n",
    "}\n",
    "\n",
    "logger = TXTLogger(work_dir=\"./logs\")\n",
    "trainer = Trainer(model, trainer_config, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [7] at entry 0 and [5] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\mipt\\Магистратура\\2 семестр\\Глубинное обучение\\ДЗ\\3\\м\\pytorch-machine-translation\\src\\models\\trainer.py:18\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, train_dataloader, val_dataloader)\u001b[0m\n\u001b[0;32m     16\u001b[0m train_epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 18\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_epoch_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda3\\envs\\ML311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda3\\envs\\ML311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Programs\\Anaconda3\\envs\\ML311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda3\\envs\\ML311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programs\\Anaconda3\\envs\\ML311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda3\\envs\\ML311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Programs\\Anaconda3\\envs\\ML311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32md:\\Programs\\Anaconda3\\envs\\ML311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [7] at entry 0 and [5] at entry 1"
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9e1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd4b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76debff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215bc2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356f15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fd36a2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Seq2SeqTransformer.__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m model_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_encoder_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_decoder_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-4\u001b[39m\n\u001b[0;32m      9\u001b[0m }\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Seq2SeqTransformer.__init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    'num_encoder_layers': 3,\n",
    "    'num_decoder_layers': 3,\n",
    "    'emb_size': 512,\n",
    "    'nhead': 8,\n",
    "    'vocab_size': max(source_tokenizer.get_vocab_size(), target_tokenizer.get_vocab_size()),\n",
    "    'max_len': 50,\n",
    "    'learning_rate': 1e-4\n",
    "}\n",
    "model = Seq2SeqTransformer(**model_config, device=data_manager.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67724383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb1587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc315dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7652773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ed321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac65ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131aa9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab198d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a7ce070-1e45-47ac-a07a-c36b6baa114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data.utils import TextUtils, short_text_filter_function\n",
    "\n",
    "# filename = 'data/rus.txt'\n",
    "# pairs = TextUtils.read_langs_pairs_from_file(filename)\n",
    "# filtered_pairs = [pair for pair in pairs if short_text_filter_function(pair, 15)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89d3c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.trainer import Trainer\n",
    "from src.models.seq2seq_transformer import Seq2SeqTransformer\n",
    "from src.data.datamodule import DataManager\n",
    "from src.data.bpe_tokenizer import BPETokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82970dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены: [0, 9, 6314, 0, 257, 171, 70, 0]\n",
      "Декодированное предложение: e llo how are you\n"
     ]
    }
   ],
   "source": [
    "# Загружаем токенизатор\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"trained_tokenizer.json\")\n",
    "\n",
    "# Функция для токенизации и детокенизации предложения\n",
    "def test_tokenizer(tokenizer, sentence):\n",
    "    encoded = tokenizer.encode(sentence)\n",
    "    print(\"Токены:\", encoded.ids)\n",
    "    decoded = tokenizer.decode(encoded.ids)\n",
    "    print(\"Декодированное предложение:\", decoded)\n",
    "\n",
    "# Тестируем на примере\n",
    "test_sentence = \"Hello, how are you?\"\n",
    "test_tokenizer(tokenizer, test_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a07c4722",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.Tokenizer' object has no attribute 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_num\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memb_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnhead\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_encoder_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_decoder_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_feedforward\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_vocab_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mget_vocab()),\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtgt_vocab_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_vocab()),\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/rus.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.8\u001b[39m\n\u001b[0;32m     17\u001b[0m }\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tokenizers.Tokenizer' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'epoch_num': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'emb_size': 512,\n",
    "    'nhead': 8,\n",
    "    'num_encoder_layers': 3,\n",
    "    'num_decoder_layers': 3,\n",
    "    'dim_feedforward': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'src_vocab_size': len(tokenizer.tokenizer.get_vocab()),\n",
    "    'tgt_vocab_size': len(tokenizer.tokenizer.get_vocab()),\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'filename': 'data/rus.txt',\n",
    "    'max_length': 15,\n",
    "    'train_size': 0.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd7501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec86bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.bpe_tokenizer import BPETokenizer\n",
    "\n",
    "# Объединяем английские и русские предложения для обучения токенизатора\n",
    "all_sentences = english + russian\n",
    "\n",
    "# Создаем и обучаем BPE токенизатор\n",
    "bpe_tokenizer = BPETokenizer(all_sentences)\n",
    "\n",
    "# Сохраняем токенизатор для последующего использования\n",
    "bpe_tokenizer.tokenizer.save(\"trained_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a68f6079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены: [0, 9, 6314, 0, 257, 171, 70, 0]\n",
      "Декодированное предложение: e llo how are you\n"
     ]
    }
   ],
   "source": [
    "# Загружаем токенизатор\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"trained_tokenizer.json\")\n",
    "\n",
    "# Функция для токенизации и детокенизации предложения\n",
    "def test_tokenizer(tokenizer, sentence):\n",
    "    encoded = tokenizer.encode(sentence)\n",
    "    print(\"Токены:\", encoded.ids)\n",
    "    decoded = tokenizer.decode(encoded.ids)\n",
    "    print(\"Декодированное предложение:\", decoded)\n",
    "\n",
    "# Тестируем на примере\n",
    "test_sentence = \"Hello, how are you?\"\n",
    "test_tokenizer(tokenizer, test_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80f21c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3276f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'epoch_num': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'src_vocab_size': len(bpe_tokenizer.tokenizer.get_vocab()),\n",
    "    'tgt_vocab_size': len(bpe_tokenizer.tokenizer.get_vocab()),\n",
    "    'emb_size': 512,\n",
    "    'nhead': 8,\n",
    "    'num_encoder_layers': 3,\n",
    "    'num_decoder_layers': 3,\n",
    "    'dim_feedforward': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee745de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_data, tgt_data):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = torch.tensor(self.src_data[idx])\n",
    "        tgt = torch.tensor(self.tgt_data[idx])\n",
    "        return src, tgt\n",
    "\n",
    "# Создание объектов Dataset\n",
    "train_dataset = TranslationDataset(tokenized_src_train, tokenized_tgt_train)\n",
    "val_dataset = TranslationDataset(tokenized_src_val, tokenized_tgt_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cefa83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.trainer import Trainer\n",
    "from src.models.seq2seq_transformer import Seq2SeqTransformer\n",
    "\n",
    "# Инициализация модели\n",
    "model = Seq2SeqTransformer(\n",
    "    num_encoder_layers=config['num_encoder_layers'],\n",
    "    num_decoder_layers=config['num_decoder_layers'],\n",
    "    emb_size=config['emb_size'],\n",
    "    nhead=config['nhead'],\n",
    "    src_vocab_size=config['src_vocab_size'],\n",
    "    tgt_vocab_size=config['tgt_vocab_size'],\n",
    "    dim_feedforward=config['dim_feedforward'],\n",
    "    dropout=config['dropout']\n",
    ").to(config['device'])\n",
    "\n",
    "# Обучение модели\n",
    "trainer = Trainer(model=model, model_config=config)\n",
    "trainer.train(train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eba516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9aaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "245297ae-d62b-4227-9312-ccff5a1c13c9",
   "metadata": {},
   "source": [
    "### Fine-tune pretrained T5 (25 баллов)\n",
    "\n",
    "Реализуйте Seq2seq Pretrained T5. Воспользуйтесь https://huggingface.co/docs/transformers/model_doc/t5 предобученной моделью. В качестве максимальной длинны возьмите предложения длинной **до 15 слов**, без каких либо префиксов. Архитектура модели(количетсво слоев, размерность и тд) остается на ваш выбор.\n",
    "\n",
    "Не забудьте важные аспекты обучения модели:\n",
    "* Взять готовый t5 токенизатор\n",
    "* Resize matrix embedding - скорей всего ваша матрица эмбеддингов не будет включать эмбеддинги из вашего сета. Пример обновления матрицы эмбеддингов тут тут https://github.com/runnerup96/Transformers-Tuning/blob/main/t5_encoder_decoder.py\n",
    "* Learning rate schedualer/Adafactor with constant learning rate\n",
    "\n",
    "\n",
    "В качестве результатов, приложите слудующие данные:\n",
    "1) Параметры обучения - learning rate, batch_size, epoch_num, pretrained model name\n",
    "2) Графики обучения - train loss, val loss, bleu score\n",
    "3) Примеры переводов вашей модели(10 штук) - source text, true target text, predicted target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5993bdc9-44f6-4af8-8e46-36d4e6824ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
